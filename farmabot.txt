#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
from dotenv import load_dotenv
import gradio as gr
import pandas as pd
from sqlalchemy import create_engine
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# LangChain imports
from langchain.docstore.document import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.sql_database import SQLDatabase
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.agents.agent_types import AgentType
from langchain.tools import Tool
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory


# In[2]:


# price is a factor for our company, so we're going to use a low cost model

MODEL = "gpt-4o-mini"
db_name = "Medicines"


# In[3]:


# Load environment variables in a file called .env

load_dotenv(override=True)
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')


# In[4]:


# --- Database Connection Details ---
SERVER_NAME = "localhost"
DATABASE_NAME = "ChatbotFarmacia" # <-- Good, you've set your DB name
TABLE_NAME = "Medicines" # Note: TABLE_NAME here isn't used for the engine itself
SCHEMA_NAME = "dbo"      # Note: SCHEMA_NAME here isn't used for the engine itself
driver = "ODBC Driver 17 for SQL Server"
connection_string = f"mssql+pyodbc://{SERVER_NAME}/{DATABASE_NAME}?driver={driver}&trusted_connection=yes"


# --- Create Engine & Load Data ---
df = pd.DataFrame()
chunks = []
try:
    print(f"Connecting to DB: {DATABASE_NAME} on {SERVER_NAME}...")
    engine = create_engine(connection_string)
    sql_query = f"SELECT * FROM [dbo].[Medicines]" # Or your relevant query
    print(f"Loading data...")
    df = pd.read_sql(sql_query, engine)
    print(f"Successfully loaded {len(df)} rows.")

    # --- Split DataFrame into Chunks ---
    # This line creates the 'chunks' variable needed below
    chunks = [df.iloc[i:i+5] for i in range(0, len(df), 5)]
    print(f"Data split into {len(chunks)} chunks.")

except Exception as e:
    print(f"Error loading data or creating chunks: {e}")
# --- Create Database Engine ---
try:
    print(f"Attempting to connect to {DATABASE_NAME} on {SERVER_NAME}...")
    engine = create_engine(connection_string)
    # Optional connection test
    connection = engine.connect()
    print(f"Successfully connected to database '{DATABASE_NAME}' on '{SERVER_NAME}'.")
    connection.close()
    print("SQLAlchemy engine created.")

except Exception as e:
    print(f"Error connecting to database: {e}")
    # Handle error
    exit()

# --- The 'engine' variable created above is what you need for the SQL Agent ---

include_tables = ["Medicines", "inventory", "inventory_chorrera", "inventory_costa_del_este", "inventory_david", "inventory_el_dorado", "inventory_san_francisco",  "Stores"] # List all tables
db = SQLDatabase(engine=engine, schema="dbo", include_tables=include_tables)
# Optional: print(db.get_table_info())


# In[5]:


llm = ChatOpenAI(model=MODEL, temperature=2) # Low temp recommended for agent logic

# Add this import line, typically near the top with your other imports


# --- Your existing code ---
# llm = ChatOpenAI(...)
# db = SQLDatabase(...)
# --- End of existing code ---

# Now you can create the agent (this line should work after the import)
sql_agent = create_sql_agent(
    llm=llm, 
    db=db, 
    agent_type="openai-tools", 
    verbose=True,
    prefix="""You are an expert SQL agent for a pharmacy system. 
    
    You have access to tables including 'Stores' which contains store location information. 
    
    When asked about stores, store counts, or locations, always query the Stores table.
    When asked "how many stores", run 'SELECT COUNT(*) FROM dbo.Stores'.
    
    Always check the schema carefully before answering and provide clear, concise responses.
    """
)
print("SQL Agent created successfully.")


# In[6]:


# --- Load the DataFrame (ensure this is done before running the agent) ---
try:
    # Define the SQL query to fetch data from the "Stores" table
    query = "SELECT StoreID, StoreName, Location FROM dbo.Stores" # Select only needed columns

    # Execute the query and load the result into a DataFrame
    # Ensure 'engine' is correctly initialized with your DB connection
    stores_df = pd.read_sql(query, engine)
    print(f"Successfully loaded {len(stores_df)} stores into DataFrame.")
    # Keep only the DataFrame in memory, maybe close the engine if not needed elsewhere
    # engine.dispose()
except Exception as e:
    print(f"Error loading stores data: {e}")
    # Handle the error appropriately, maybe exit or use dummy data
    stores_df = pd.DataFrame() # Create an empty DataFrame to prevent errors later

# --- Make sure stores_df is accessible globally or passed correctly ---


# In[13]:


# Simple LangChain implementation without typing

# Basic imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.tools import tool
from langchain.memory import ChatMessageHistory

# Session storage for maintaining conversation history
session_histories = {}

# Define the models/LLMs
# Assuming MODEL is defined elsewhere in your code
llm = ChatOpenAI(model=MODEL, temperature=0.7)
agent_llm = ChatOpenAI(model=MODEL, temperature=0)

# Load the vectorstore
if 'vectorstore' not in locals() or vectorstore is None:
    print("Loading existing vectorstore from disk...")
    embeddings = OpenAIEmbeddings()
    try:
        vectorstore = Chroma(persist_directory=db_name, embedding_function=embeddings)
        print(f"Loaded vectorstore with {vectorstore._collection.count()} documents")
    except Exception as e:
        print(f"Error loading vectorstore: {e}")

# Set up the retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# Function to get or create session history
def get_session_history(session_id):
    if session_id not in session_histories:
        session_histories[session_id] = ChatMessageHistory()
    return session_histories[session_id]

# Create a RAG chain
def create_rag_chain():
    # Define the prompt template
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="""You are an expert pharmaceutical assistant. Use the following context to answer the question.
        
If you're asked about side effects, focus on the information in the 'Side Effects (Common)' and 'Side Effects (Rare)' fields.
If you're asked about stores or inventory, explain that this information needs to be queried from the database.
Answer the question based only on the provided context. If the information isn't available, say so clearly."""),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessage(content="{question}"),
        SystemMessage(content="Context: {context}")
    ])
    
    # Create the RAG chain
    return (
        {"context": retriever, "question": RunnablePassthrough(), "chat_history": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

# Create the RAG chain
rag_chain = create_rag_chain()

# Define the SQL query tool
@tool
def sql_query(query):
    """Execute a SQL query against the store and inventory database"""
    try:
        # Assuming sql_agent is defined elsewhere
        return sql_agent.invoke({"input": query})["output"]
    except Exception as e:
        return f"Error querying database: {str(e)}"

# Vector search function for direct access to RAG
def vector_search(query, session_id="default"):
    try:
        # Get history and pass it explicitly
        history = get_session_history(session_id)
        result = rag_chain.invoke({"question": query, "chat_history": history.messages})
        
        # Record the exchange
        history.add_user_message(query)
        history.add_ai_message(result)
        return result
    except Exception as e:
        print(f"Error in vector_search: {e}")
        # Fallback to basic query without history
        return rag_chain.invoke({"question": query, "chat_history": []})

# Simple agent without LangGraph
def simple_agent(query):
    """A simple agent implementation that doesn't use LangGraph"""
    # Create a prompt for the agent
    agent_prompt = f"""You are a helpful assistant that can answer questions about medicines and store inventory.

Question: {query}

If this is about store inventory, locations, or similar store-related information, use the SQL database.
If this is about medicine properties, side effects, or drug information, use the medicine information database.
Otherwise, answer directly.

Respond with your final answer."""

    # Get a response from the LLM
    response = agent_llm.invoke(agent_prompt)
    
    # Extract the content
    if hasattr(response, "content"):
        return response.content
    else:
        return str(response)

# The integrated chat function
def chat(question, session_id="default"):
    try:
        # For store-related questions, directly route to SQL agent
        if any(keyword in question.lower() for keyword in ["store", "stores", "location", "locations", "inventory", "stock", "how many"]):
            try:
                print(f"Routing to SQL agent: {question}")
                result = sql_query(question)
                
                # Record the exchange in history
                history = get_session_history(session_id)
                history.add_user_message(question)
                history.add_ai_message(result)
                return result
            except Exception as e:
                print(f"SQL direct routing failed: {e}, falling back to simple agent")
        
        # For medicine-related questions about side effects, use RAG
        if any(keyword in question.lower() for keyword in ["side effect", "medicine", "drug", "medication"]):
            try:
                print(f"Routing to RAG chain: {question}")
                return vector_search(question, session_id)
            except Exception as e:
                print(f"RAG chain failed: {e}, falling back to simple agent")
        
        # For general questions, use the simple agent
        try:
            print(f"Using simple agent: {question}")
            result = simple_agent(question)
            
            # Record the exchange in history
            history = get_session_history(session_id)
            history.add_user_message(question)
            history.add_ai_message(result)
            
            return result
        except Exception as e:
            print(f"Agent failed: {e}, falling back to direct LLM")
            try:
                response = llm.invoke(question)
                content = response.content if hasattr(response, "content") else str(response)
                return content
            except Exception as llm_err:
                return f"I encountered several errors processing your request. Please try rephrasing your question. Error: {str(llm_err)}"
                
    except Exception as e:
        return f"I encountered an error: {str(e)}. Please try rephrasing your question."


# In[14]:


def get_store_count() -> str:
  """
  Use this tool ONLY when asked about the total number, count, quantity, or amount of store locations the company has.
  Returns the total count as a string.
  """
  global stores_df # Access the DataFrame (or pass it in if preferred)
  if stores_df is None or stores_df.empty:
      return "I cannot access the store data right now to determine the count."
  num_stores = len(stores_df)
  return f"There are currently {num_stores} store locations."

# Create a list of tools for the agent
tools = [get_store_count]


# In[15]:


# Define the SQL query to fetch data from the "Stores" table
query = "SELECT * FROM dbo.Stores"

# Execute the query and load the result into a DataFrame
stores_df = pd.read_sql(query, engine)
print(stores_df.head())  # Print the first few rows to inspect the data



# In[19]:


# --- Corrected Document Creation Loop (Option 1) ---
# Assuming 'chunks' is your list of DataFrames from the SQL query
# Requires: from langchain.docstore.document import Document

docs = []
print("Starting document conversion (translating 1/0 status to text in page_content)...")
for i, chunk_df in enumerate(chunks):
    for index, row in chunk_df.iterrows():
        try:
            # --- Get the numeric status (assuming column name is 'Prescription') ---
            try:
                 # Use the actual column name from your SQL table if different from 'Prescription'
                 status_flag = int(row.get('Prescription', -1)) # Get 1, 0, or -1
            except (ValueError, TypeError):
                 status_flag = -1 # Handle non-numeric or missing data

            # --- Translate numeric status to text ---
            if status_flag == 1:
                status_text = "Requires Prescription"
            elif status_flag == 0:
                status_text = "Over-the-Counter"
            else:
                status_text = "Unknown"

            # --- MODIFIED page_content to include the status TEXT ---
            # Use correct column names from your SQL table (e.g., 'Generic Name', 'Uses')
            page_content = f"Medicine: {row['Generic Name']}\nUses: {row['Uses']}\nPrescription Status: {status_text}"

            # --- Metadata: Store the numeric flag and other relevant fields ---
            metadata = {
                "source_db_table": f"{SCHEMA_NAME}.{TABLE_NAME}", # Identify source
                # Use primary key from DB if available and useful, otherwise use index
                # "db_primary_key": row.get('YourPrimaryKeyColumn'),
                "chunk_index": i,
                # Store the numeric flag using a clear key name
                "prescription_required_flag": status_flag,
                # Add other relevant fields from your DB table, ensuring column names match
                "uses": row.get('Uses', ""),
                "side_effects_common": row.get('Side Effects (Common)', ""),
                "side_effects_rare": row.get('Side Effects (Rare)', ""),
                "similar_drugs": row.get('Similar Drugs', ""),
                "brand_name_1": row.get('Brand Name 1', ""),
                # ... etc
            }
            docs.append(Document(page_content=page_content, metadata=metadata))
        except KeyError as e:
            print(f"KeyError processing row {index}: {e} - Check column names from DB query!")
        except Exception as e:
             print(f"Error processing row {index}: {e}")

print(f"Created {len(docs)} Document objects with updated page_content.")
# --- End of Corrected Document Creation Loop ---


# In[20]:


query = "How many stores are there?"
result = sql_agent.invoke({"input": query})
print(result.get("output", "No output provided."))


# In[21]:


# Optionally, view the first chunk
print(len(chunks))


# In[23]:


def get_store_count() -> str:
  """
  Use this tool ONLY when asked about the total number, count, quantity, or amount of store locations the company has.
  Returns the total count as a string.
  """
  global stores_df # Access the DataFrame (or pass it in if preferred)
  if stores_df is None or stores_df.empty:
      return "I cannot access the store data right now to determine the count."
  num_stores = len(stores_df)
  return f"There are currently {num_stores} store locations."

# Create a list of tools for the agent
tools = [get_store_count]


# In[26]:


# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk
# Chroma is a popular open source Vector Database based on SQLLite

embeddings = OpenAIEmbeddings() # Assumes 'from langchain_openai import OpenAIEmbeddings' was used
                               # and the OpenAI API key is configured (e.g., environment variable)

# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers
# Then replace embeddings = OpenAIEmbeddings()
# with:
# from langchain.embeddings import HuggingFaceEmbeddings
# embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Delete if already exists
# Assumes 'db_name' variable (string path) is defined earlier
# Assumes 'import os' and 'from langchain_chroma import Chroma' were used

if os.path.exists(db_name):
    try:
        # Attempt to connect and delete the collection within the directory
        Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()
        print(f"Deleted existing collection in '{db_name}'.")
    except Exception as e:
        # Handle cases where deletion might fail (e.g., directory exists but isn't a valid Chroma DB)
        print(f"Could not delete collection in '{db_name}': {e}")

# Create vectorstore

vectorstore = Chroma.from_documents(documents=docs, # 'docs' needs to be List[Document]
                                     embedding=embeddings,
                                     persist_directory=db_name)
print(f"Vectorstore created with {vectorstore._collection.count()} documents in '{db_name}'.")


# In[28]:


vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=db_name)
print(f"Vectorstore created with {vectorstore._collection.count()} documents")


# In[31]:


# Let's investigate the vectors

collection = vectorstore._collection
count = collection.count()

sample_embedding = collection.get(limit=1, include=["embeddings"])["embeddings"][0]
dimensions = len(sample_embedding)
print(f"There are {count:,} vectors with {dimensions:,} dimensions in the vector store")


# In[32]:


result = collection.get(include=['embeddings', 'documents', 'metadatas'])
vectors = np.array(result['embeddings']) # Requires: import numpy as np
documents = result['documents']
metadatas = result['metadatas']
# doc_types = [metadata['doc_type'] for metadata in metadatas if metadata is not None] # REMOVED/COMMENTED

# You can now work with vectors, documents, metadatas
print(f"Retrieved {len(vectors)} items.")
if metadatas:
     print("First item metadata:", metadatas[0])


# In[33]:


# Define the model name as a string
MODEL = "gpt-4"

# Import the modern components
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Create a Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)
# Alternative - if you'd like to use Ollama locally, uncomment this line instead
# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')

# The retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# Create a message history store
message_histories = {}

# Function to get or create a message history for a session
def get_message_history(session_id):
    if session_id not in message_histories:
        message_histories[session_id] = ChatMessageHistory()
    return message_histories[session_id]

# Create the modern conversation chain
def create_conversation_chain():
    # Define the prompt
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="You are a helpful assistant. Answer based on the retrieved context."),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessage(content="{question}"),
        SystemMessage(content="Context: {context}")
    ])
    
    # Create the chain
    chain = (
        {"context": retriever, "question": lambda x: x["question"]}
        | prompt
        | llm
    )
    
    # Wrap with history management
    return RunnableWithMessageHistory(
        chain,
        get_message_history,
        input_messages_key="question",
        history_messages_key="chat_history"
    )

# Create the conversation chain
conversation_chain = create_conversation_chain()

# Function to query the chain
def query(question, session_id="default"):
    return conversation_chain.invoke(
        {"question": question},
        {"configurable": {"session_id": session_id}}
    )


# In[36]:


# Set up a simpler implementation focused on clarity
from langchain_core.callbacks import StdOutCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# Create a callback handler
handler = StdOutCallbackHandler()

# Initialize the LLM with callbacks
llm = ChatOpenAI(
    temperature=0.7, 
    model_name=MODEL, 
    callbacks=[handler]
)

# Function to directly query about headache medications
def direct_query(question):
    # Get documents from retriever
    docs = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # Create a simple, explicit prompt
    messages = [
        SystemMessage(content="You are a helpful assistant that answers questions about medications."),
        HumanMessage(content=f"""
I need information about medications for headaches.

Here is my question: {question}

Here is information from our medical database to help you answer:
{context}

Please provide a comprehensive answer based on this information.
""")
    ]
    
    # Get response from LLM
    response = llm.invoke(messages)
    return response.content

# Execute the query
query = "What medicines are good for headaches?"
print("Sending query:", query)
result = direct_query(query)
print("\nAnswer:", result)


# In[38]:


# create a new Chat with OpenAI
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use
retriever = vectorstore.as_retriever(search_kwargs={"k": 25})

# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)


# In[41]:


def chat(question, history):
    print("Using SQL agent-enabled chat function")  # Debug print
    try:
        response = run_sql_agent(question, sql_agent)
        history = history + [(question, response)]
        return response, history
    except Exception as e:
        error_msg = f"Error during SQL agent invocation: {str(e)}"
        print(error_msg)
        return error_msg, history



# In[42]:


# Define your SQL agent and run_sql_agent before launching the interface
def run_sql_agent(query, sql_agent):
    try:
        result = sql_agent.invoke({"input": query})
        output = result.get("output", "No output provided.")
        return output
    except Exception as e:
        return f"Error querying database: {str(e)}"

def chat(question, history):
    print("Using SQL agent-enabled chat function")
    try:
        print(f"Sending question to SQL agent: {question}")
        response = run_sql_agent(question, sql_agent)
        print(f"Received response: {response[:100]}...")  # Print first 100 chars
        history = history + [(question, response)]
        return response, history
    except Exception as e:
        error_msg = f"Error during SQL agent invocation: {str(e)}"
        print(error_msg)
        import traceback
        print(traceback.format_exc())  # Print full traceback
        return error_msg, history


# In[45]:


query = "How many stores are there?"
response = run_sql_agent(query, sql_agent)
print(response)


# In[46]:


# Integrated chat function that combines RAG and SQL capabilities
def chat(message_list):
    """
    Handles a list of message dictionaries and routes to appropriate system:
    - SQL queries go to the SQL agent
    - Medication questions go to the RAG system
    - Other queries get a direct response
    
    Args:
        message_list: List of message dictionaries with 'role' and 'content' keys
        
    Returns:
        String response to the user
    """
    print("Received message list:", message_list)  # Debug print
    
    if not message_list:
        return "Say something!"
    
    # Get the content of the last message (which should be from the user)
    last_message = message_list[-1]
    user_content = last_message.get('content', '')  # Use .get for safety
    
    # Store previous messages to build conversation context
    previous_messages = []
    for msg in message_list[:-1]:  # All messages except the last one
        if msg.get('role') in ['user', 'assistant']:
            previous_messages.append(f"{msg.get('role')}: {msg.get('content', '')}")
    
    conversation_context = "\n".join(previous_messages)
    print(f"Conversation context length: {len(conversation_context)} characters")
    
    # Determine which system to use based on query content
    # 1. SQL-related queries
    sql_keywords = ['store', 'inventory', 'location', 'how many', 'where', 'stock', 
                   'database', 'query', 'find stores', 'nearest', 'available']
    
    # 2. Medicine-related queries
    medicine_keywords = ['medicine', 'drug', 'medication', 'prescription', 'side effect', 
                        'dosage', 'treatment', 'headache', 'pain', 'symptom']
    
    # Check if the query matches SQL patterns
    if any(keyword in user_content.lower() for keyword in sql_keywords):
        print(f"Routing to SQL agent: {user_content}")
        try:
            # Assuming sql_agent is defined elsewhere
            result = sql_agent.invoke({"input": user_content})
            response = result.get("output", "I couldn't find that information in our store database.")
            print(f"SQL response: {response[:100]}...")  # Print first 100 chars
            return response
        except Exception as e:
            error_msg = f"I encountered an error querying the store database: {str(e)}"
            print(error_msg)
            # Fall back to RAG if SQL fails
            print("Falling back to general knowledge...")
    
    # Check if the query matches medicine patterns
    if any(keyword in user_content.lower() for keyword in medicine_keywords):
        print(f"Routing to RAG system: {user_content}")
        try:
            # Assuming conversation_chain is your RAG chain
            result = conversation_chain.invoke({"question": user_content})
            response = result.get("answer", "I couldn't find information about that medication.")
            print(f"RAG response: {response[:100]}...")  # Print first 100 chars
            return response
        except Exception as e:
            error_msg = f"I encountered an error retrieving medication information: {str(e)}"
            print(error_msg)
            # Fall back to direct response
    
    # Default response for general queries
    print("Using default response generation")
    try:
        # Use the LLM directly for general conversation
        from langchain_core.messages import HumanMessage, SystemMessage
        
        messages = [
            SystemMessage(content=f"""You are a helpful pharmacy assistant. 
            Be conversational and friendly.
            
            Previous conversation:
            {conversation_context}"""),
            HumanMessage(content=user_content)
        ]
        
        # Assuming llm is defined elsewhere
        response = llm.invoke(messages).content
        return response
    except Exception as e:
        # Ultimate fallback
        return f"I understand you said: '{user_content}', but I'm having trouble generating a response right now. How else can I help you?"


# In[ ]:


import gradio as gr
import time

# Simplified chat function for testing
def simple_chat(message, history):
    """Simple chat function that echoes the message for testing the interface"""
    # Just echo the message back for testing
    print(f"Received message: {message}")
    print(f"History: {history}")
    
    # Return in the format expected by the messages-type Chatbot
    return history + [
        {"role": "user", "content": message},
        {"role": "assistant", "content": f"You said: {message}"}
    ]

# Create a very basic Gradio interface with just the essential components
def create_basic_interface():
    """Creates a minimal Gradio interface just to verify components are working"""
    
    with gr.Blocks() as demo:
        gr.Markdown("# Pharmacy Assistant")
        
        # Chat interface with just the core components
        chatbot = gr.Chatbot(type="messages")
        
        # Input box and send button in their own row to ensure visibility
        with gr.Row():
            msg = gr.Textbox(
                placeholder="Type your message here...",
                show_label=False,
                scale=4
            )
            submit = gr.Button("Send", scale=1)
        
        # Simple event handling
        submit.click(
            fn=simple_chat,
            inputs=[msg, chatbot],
            outputs=chatbot
        ).then(
            fn=lambda: "",
            inputs=None,
            outputs=msg
        )
        
        msg.submit(
            fn=simple_chat,
            inputs=[msg, chatbot],
            outputs=chatbot
        ).then(
            fn=lambda: "",
            inputs=None,
            outputs=msg
        )
    
    return demo

# Launch the interface
if __name__ == "__main__":
    demo = create_basic_interface()
    # Set debug=True to get more information about the interface
    demo.launch(debug=True)


# In[ ]:




