{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fac9612-f230-4259-8fe5-1a70f10c5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import traceback\n",
    "import logging\n",
    "import time # Although time wasn't used in the final snippets, it was in logging setup\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np # Although numpy wasn't used in final snippets, keep if needed for Chroma internals etc.\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# LangChain Core Imports\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.callbacks import StdOutCallbackHandler # Used in one of the LLM initializations\n",
    "\n",
    "# LangChain Community Imports\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory # Used for Gradio state potentially\n",
    "\n",
    "# LangChain OpenAI Imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI, OpenAI # OpenAI used for classification example\n",
    "\n",
    "# LangChain Integrations / Main Package Imports\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.chains import ConversationalRetrievalChain # Used in one example chain\n",
    "from langchain.memory import ChatMessageHistory as LangchainChatMessageHistory # Used in one example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49828431-91f7-48b0-83ab-defa0cced9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16aa0203-0502-4d0c-8427-9e1f236506d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Database Connection Details ---\n",
    "# Model & DB Configuration\n",
    "MODEL = \"gpt-4-turbo\" # Or \"gpt-4\" as used in some later cells\n",
    "DB_NAME = \"Medicines\" # This wasn't used directly, DATABASE_NAME is used\n",
    "SERVER_NAME = \"localhost\"\n",
    "DATABASE_NAME = \"ChatbotFarmacia\"\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "SCHEMA_NAME = \"dbo\" # Used in SQLDatabase and document metadata\n",
    "TABLE_NAME = \"Medicines\" # Used in document metadata\n",
    "VECTOR_DB_PATH = \"medicines_vectordb\" # Renamed from db_name for clarity\n",
    "LOG_FILE = 'farmabot_logs.log'\n",
    "\n",
    "# Included tables for SQL Agent\n",
    "INCLUDE_TABLES = [\n",
    "    \"Medicines\", \"inventory\", \"inventory_chorrera\",\n",
    "    \"inventory_costa_del_este\", \"inventory_david\", \"inventory_el_dorado\",\n",
    "    \"inventory_san_francisco\", \"Stores\"\n",
    "]\n",
    "\n",
    "# Store names for keyword matching (ensure consistency with DB/inventory tables)\n",
    "STORE_NAMES_ES = [\"chorrera\", \"costa del este\", \"david\", \"el dorado\", \"san francisco\"]\n",
    "STORE_NAMES_EN = [\"chorrera\", \"costa del este\", \"david\", \"el dorado\", \"san francisco\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9bf839-fd02-45ca-a24c-e5988513c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Logging Setup\n",
    "# ==============================================================================\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def log_interaction(query, response, query_type, duration_ms=0, error=None):\n",
    "    \"\"\"Log a single user interaction to file and console.\"\"\"\n",
    "    log_data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"response_length\": len(str(response)), # Ensure response is string for len()\n",
    "        \"duration_ms\": duration_ms,\n",
    "        \"error\": str(error) if error else None\n",
    "    }\n",
    "    log_message = f\"INTERACTION: {log_data}\"\n",
    "    print(f\"LOG: {log_message}\") # Console log\n",
    "    logging.info(log_message) # File log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c033c2d2-a8bb-4670-a947-a8fcafb0c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to ChatbotFarmacia on localhost...\n",
      "Successfully connected to database 'ChatbotFarmacia' on 'localhost'.\n",
      "Loading medicine data...\n",
      "Successfully loaded 627 medicine rows.\n",
      "Medicine data split into 126 chunks.\n",
      "Loading stores data...\n",
      "Successfully loaded 5 stores into DataFrame.\n",
      "SQLDatabase interface created.\n"
     ]
    }
   ],
   "source": [
    "# Database Connection & Initial Data Load\n",
    "# ==============================================================================\n",
    "connection_string = f\"mssql+pyodbc://{SERVER_NAME}/{DATABASE_NAME}?driver={DRIVER}\"\n",
    "engine = None\n",
    "db = None\n",
    "df_medicines = pd.DataFrame() # Use a more descriptive name\n",
    "chunks = []\n",
    "stores_df = pd.DataFrame() # For the get_store_count tool if kept\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to connect to {DATABASE_NAME} on {SERVER_NAME}...\")\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Test the connection\n",
    "    with engine.connect() as connection:\n",
    "        print(f\"Successfully connected to database '{DATABASE_NAME}' on '{SERVER_NAME}'.\")\n",
    "\n",
    "    # Load initial Medicines data\n",
    "    sql_query_medicines = f\"SELECT * FROM [{SCHEMA_NAME}].[{TABLE_NAME}]\"\n",
    "    print(f\"Loading medicine data...\")\n",
    "    df_medicines = pd.read_sql(sql_query_medicines, engine)\n",
    "    print(f\"Successfully loaded {len(df_medicines)} medicine rows.\")\n",
    "\n",
    "    # Split DataFrame into Chunks\n",
    "    chunks = [df_medicines.iloc[i:i+5] for i in range(0, len(df_medicines), 5)]\n",
    "    print(f\"Medicine data split into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Load Stores data (potentially for get_store_count tool)\n",
    "    sql_query_stores = f\"SELECT StoreID, StoreName, Location FROM {SCHEMA_NAME}.Stores\"\n",
    "    print(\"Loading stores data...\")\n",
    "    stores_df = pd.read_sql(sql_query_stores, engine)\n",
    "    print(f\"Successfully loaded {len(stores_df)} stores into DataFrame.\")\n",
    "\n",
    "    # Setup LangChain SQLDatabase interface\n",
    "    db = SQLDatabase(engine=engine, schema=SCHEMA_NAME, include_tables=INCLUDE_TABLES)\n",
    "    print(\"SQLDatabase interface created.\")\n",
    "    # Optional: print(db.get_table_info())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Error connecting to database or loading initial data: {e}\")\n",
    "    log_interaction(query=\"DB Setup\", response=\"Error\", query_type=\"db_setup_error\", error=e)\n",
    "    # Handle error appropriately - maybe exit or prevent chatbot launch\n",
    "    engine = None\n",
    "    db = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644935d4-e098-45bd-a3ca-686adca45a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM and Embeddings models initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use one consistent LLM for the main chat and agent, unless specific temps are needed\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.5) # Adjusted temperature slightly\n",
    "embeddings = OpenAIEmbeddings()\n",
    "print(\"LLM and Embeddings models initialized.\")\n",
    "# --- Updated SQL Agent Creation ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99abe597-e50f-4aff-9678-385496262bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document conversion for vector store...\n",
      "Error processing row 198 for vector store: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "Created 626 Document objects.\n",
      "Attempting to delete existing vector store collection in 'medicines_vectordb'...\n",
      "Deleted existing collection in 'medicines_vectordb'.\n",
      "Creating new vector store...\n",
      "Vector store created with 626 documents in 'medicines_vectordb'.\n",
      "Retriever created from vector store.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Vector Store Setup\n",
    "# ==============================================================================\n",
    "vectorstore = None\n",
    "if not df_medicines.empty:\n",
    "    docs = []\n",
    "    print(\"Starting document conversion for vector store...\")\n",
    "    for i, chunk_df in enumerate(chunks):\n",
    "        for index, row in chunk_df.iterrows():\n",
    "            try:\n",
    "                status_flag = int(row.get('Prescription', -1))\n",
    "                status_text = \"Requires Prescription\" if status_flag == 1 else \"Over-the-Counter\" if status_flag == 0 else \"Unknown\"\n",
    "                page_content = f\"Medicine: {row['Generic Name']}\\nUses: {row['Uses']}\\nPrescription Status: {status_text}\"\n",
    "                metadata = {\n",
    "                    \"source_db_table\": f\"{SCHEMA_NAME}.{TABLE_NAME}\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"prescription_required_flag\": status_flag,\n",
    "                    \"uses\": row.get('Uses', \"\"),\n",
    "                    \"side_effects_common\": row.get('Side Effects (Common)', \"\"),\n",
    "                    \"side_effects_rare\": row.get('Side Effects (Rare)', \"\"),\n",
    "                    \"similar_drugs\": row.get('Similar Drugs', \"\"),\n",
    "                    \"brand_name_1\": row.get('Brand Name 1', \"\"),\n",
    "                    # Add other relevant metadata fields...\n",
    "                }\n",
    "                docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError processing row {index} for vector store: {e} - Check column names!\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing row {index} for vector store: {e}\")\n",
    "    print(f\"Created {len(docs)} Document objects.\")\n",
    "\n",
    "    # Delete old vector store if exists\n",
    "    if os.path.exists(VECTOR_DB_PATH):\n",
    "        try:\n",
    "            print(f\"Attempting to delete existing vector store collection in '{VECTOR_DB_PATH}'...\")\n",
    "            Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings).delete_collection()\n",
    "            print(f\"Deleted existing collection in '{VECTOR_DB_PATH}'.\")\n",
    "            # Consider removing the directory itself if needed: shutil.rmtree(VECTOR_DB_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not delete collection in '{VECTOR_DB_PATH}', may attempt overwrite: {e}\")\n",
    "\n",
    "    # Create new vector store\n",
    "    try:\n",
    "        print(\"Creating new vector store...\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=VECTOR_DB_PATH\n",
    "        )\n",
    "        print(f\"Vector store created with {vectorstore._collection.count()} documents in '{VECTOR_DB_PATH}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not create vector store: {e}\")\n",
    "        log_interaction(query=\"Vector Store Setup\", response=\"Error\", query_type=\"vectorstore_error\", error=e)\n",
    "        vectorstore = None # Ensure it's None if creation fails\n",
    "else:\n",
    "    print(\"WARNING: Medicine DataFrame is empty, skipping vector store creation.\")\n",
    "\n",
    "# Setup retriever (only if vectorstore was created)\n",
    "retriever = None\n",
    "if vectorstore:\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # Retrieve top 5 chunks\n",
    "    print(\"Retriever created from vector store.\")\n",
    "else:\n",
    "    print(\"WARNING: Vector store not available, RAG functionality will be disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad671f8a-aef1-45b1-a211-8935c3dc9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching table info for SQL Agent prompt...\n",
      "Table info fetched successfully.\n",
      "Creating SQL Agent...\n",
      "SQL Agent created successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SQL Agent & Tools Setup\n",
    "# ==============================================================================\n",
    "sql_agent = None\n",
    "# Define a placeholder for the tool even if agent creation fails initially\n",
    "sql_query_tool_placeholder = None\n",
    "\n",
    "if db and llm: # Ensure db and llm objects were created successfully\n",
    "    try:\n",
    "        # --- Pre-calculate table info to avoid complex f-string evaluation issues ---\n",
    "        print(\"Fetching table info for SQL Agent prompt...\")\n",
    "        stores_table_info = db.get_table_info(['Stores'])\n",
    "        medicines_table_info = db.get_table_info(['Medicines'])\n",
    "        print(\"Table info fetched successfully.\")\n",
    "\n",
    "        # --- Define the agent prefix using the pre-calculated info ---\n",
    "        agent_prefix = f\"\"\"You are an expert SQL agent for a pharmacy system.\n",
    "\n",
    "        You have access to tables including 'Stores' (schema: {stores_table_info}) which contains store location information,\n",
    "        and various 'inventory_...' tables (like 'inventory_chorrera', 'inventory_costa_del_este', etc.) containing stock levels for medicines.\n",
    "        The primary medicine information is in the 'Medicines' table (schema: {medicines_table_info}).\n",
    "\n",
    "        When asked about stores, store counts, or general locations, query the 'Stores' table.\n",
    "        When asked \"how many stores\", run 'SELECT COUNT(*) FROM {SCHEMA_NAME}.Stores'.\n",
    "\n",
    "        IMPORTANT: When asked about inventory, stock, quantity, or availability for a specific medicine:\n",
    "        1. Identify the medicine name precisely.\n",
    "        2. Identify the specific store location if mentioned (e.g., 'Chorrera', 'Costa del Este'). Try to map these common names to the relevant inventory table (like '{SCHEMA_NAME}.inventory_chorrera') or filter based on store name if querying a unified inventory table.\n",
    "        3. Query the appropriate table(s) to find the current quantity for that medicine at that location (or overall if no location is specified).\n",
    "        4. Return the quantity clearly as a number if possible. If reporting for a specific store, mention it. Example response: \"There are 48 units of Tylenol in stock at the Costa del Este branch.\"\n",
    "\n",
    "        Always check the schema carefully before answering and provide clear, concise responses based ONLY on the database information.\n",
    "        Do not make up information. If you cannot find the information, say so clearly.\n",
    "        \"\"\"\n",
    "        # --- Create the SQL Agent ---\n",
    "        print(\"Creating SQL Agent...\")\n",
    "        sql_agent = create_sql_agent(\n",
    "            llm=llm,\n",
    "            db=db, # Pass the db object itself here\n",
    "            agent_type=\"openai-tools\",\n",
    "            verbose=True,\n",
    "            prefix=agent_prefix # Use the pre-formatted prefix\n",
    "        )\n",
    "        print(\"SQL Agent created successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not create SQL Agent: {e}\")\n",
    "        # Log the full traceback for better debugging if the issue persists\n",
    "        import traceback\n",
    "        print(\"Traceback:\")\n",
    "        print(traceback.format_exc())\n",
    "        log_interaction(query=\"SQL Agent Setup\", response=\"Error\", query_type=\"sql_agent_error\", error=traceback.format_exc()) # Log full traceback\n",
    "        sql_agent = None # Ensure agent is None if creation fails\n",
    "else:\n",
    "    print(\"WARNING: SQL Database interface or LLM not available, SQL Agent cannot be created.\")\n",
    "\n",
    "# --- Define SQL Query Tool ---\n",
    "# Define the tool function regardless of whether the agent was created successfully.\n",
    "# It will check for the agent's existence internally when called.\n",
    "@tool\n",
    "def sql_query(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against the store and inventory database. Use this for questions about store locations, store counts, and specific medicine inventory/stock levels.\"\"\"\n",
    "    if not sql_agent:\n",
    "         print(\"WARNING: sql_query tool called, but SQL Agent is not available.\")\n",
    "         # Provide a user-friendly message if the agent isn't ready\n",
    "         # Consider checking language preference here if possible, otherwise default\n",
    "         return \"Lo siento, no puedo acceder a la base de datos de la tienda en este momento. / Sorry, I cannot access the store database right now.\"\n",
    "    try:\n",
    "        print(f\"DEBUG: Sending query to SQL Agent: {query}\") # Debug print\n",
    "        result = sql_agent.invoke({\"input\": query})\n",
    "        # Handle potential variations in agent output structure\n",
    "        output = result.get(\"output\", result.get(\"result\", \"No specific output found.\"))\n",
    "        print(f\"DEBUG: Received output from SQL Agent: {output}\") # Debug print\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking SQL agent for query '{query}': {e}\")\n",
    "        import traceback\n",
    "        log_interaction(query=f\"SQL Agent Query: {query}\", response=\"Error\", query_type=\"sql_agent_query_error\", error=traceback.format_exc())\n",
    "        # Provide a user-friendly error message\n",
    "        return f\"Error al consultar la base de datos. / Error querying database.\"\n",
    "\n",
    "# Assign the function to the placeholder variable (optional, but can be useful)\n",
    "sql_query_tool_placeholder = sql_query\n",
    "\n",
    "# (Optional) Define get_store_count tool - Commented out as likely redundant\n",
    "# @tool\n",
    "# def get_store_count() -> str:\n",
    "#     # ... (implementation) ...\n",
    "\n",
    "def chat(message_list):\n",
    "    \"\"\"\n",
    "    Handles messages, routes requests, respects language, focuses on pharmacy topics,\n",
    "    handles inventory/orders, attempts context carry-over, translates SQL responses,\n",
    "    and includes enhanced debugging for AttributeError.\n",
    "    \"\"\"\n",
    "    print(\"DEBUG: Received message list:\", message_list)\n",
    "\n",
    "    # --- Outer Try-Except for general errors ---\n",
    "    try:\n",
    "        if not message_list:\n",
    "            return \"Say something!\"\n",
    "\n",
    "        # --- Language Detection & Last Messages ---\n",
    "        language = \"es\" # Default\n",
    "        # Ensure message_list[0] exists and is a dictionary before accessing\n",
    "        if message_list and isinstance(message_list[0], dict) and \\\n",
    "           message_list[0].get('role') == 'system' and 'User prefers:' in message_list[0].get('content', ''):\n",
    "             language = \"en\" if \"en\" in message_list[0].get('content') else \"es\"\n",
    "\n",
    "        user_content = \"\"\n",
    "        # Ensure user_content is always a string\n",
    "        if message_list and isinstance(message_list[-1], dict) and message_list[-1].get('role') == 'user':\n",
    "            content_val = message_list[-1].get('content', '')\n",
    "            if isinstance(content_val, str):\n",
    "                 user_content = content_val\n",
    "            else:\n",
    "                 print(f\"WARNING: User message content is not a string: {type(content_val)}. Converting.\")\n",
    "                 user_content = str(content_val)\n",
    "\n",
    "        last_bot_message = \"\"\n",
    "        if len(message_list) > 1 and isinstance(message_list[-2], dict) and message_list[-2].get('role') == 'assistant':\n",
    "             bot_content_val = message_list[-2].get('content', '')\n",
    "             if isinstance(bot_content_val, str):\n",
    "                  last_bot_message = bot_content_val\n",
    "             else:\n",
    "                  print(f\"WARNING: Bot message content is not a string: {type(bot_content_val)}. Converting.\")\n",
    "                  last_bot_message = str(bot_content_val)\n",
    "\n",
    "        if not user_content:\n",
    "            return \"Por favor, dime algo.\" if language == \"es\" else \"Please say something.\"\n",
    "\n",
    "        # --- Keyword Lists ---\n",
    "        # Define locally or ensure global definitions are accessible\n",
    "        # Using global lists defined outside the function is assumed here\n",
    "        location_keywords_es = [\"tienda\", \"tiendas\", \"farmacia\", \"farmacias\", \"sucursal\", \"sucursales\", \"ubicación\", \"ubicaciones\", \"cuántas\", \"donde\", \"dónde\", \"local\", \"locales\"] + STORE_NAMES_ES\n",
    "        location_keywords_en = [\"store\", \"stores\", \"pharmacy\", \"pharmacies\", \"location\", \"locations\", \"how many\", \"where\", \"branch\", \"branches\"] + STORE_NAMES_EN\n",
    "        hours_keywords_es = [\"hora\", \"horas\", \"horario\", \"horarios\", \"abierto\", \"cierra\", \"abre\", \"disponible\", \"atención\", \"atienden\", \"cuando\"]\n",
    "        hours_keywords_en = [\"hour\", \"hours\", \"schedule\", \"open\", \"close\", \"opens\", \"closes\", \"availability\", \"available\", \"when\", \"time\", \"timing\"]\n",
    "        medication_keywords_es = [\"medicamento\", \"medicina\", \"medicinas\", \"droga\", \"drogas\", \"pastilla\", \"efecto\", \"efectos\", \"secundarios\", \"alternativa\", \"dosis\", \"uso\", \"usos\", \"para qué sirve\", \"receta\"] + known_medicines\n",
    "        medication_keywords_en = [\"medication\", \"medicine\", \"drug\", \"drugs\", \"pill\", \"effect\", \"effects\", \"side effect\", \"alternative\", \"dose\", \"use\", \"uses\", \"what is it for\", \"prescription\"] + known_medicines\n",
    "        inventory_keywords_es = [\"inventario\", \"stock\", \"cantidad\", \"cuantos\", \"cuántos\", \"disponible\", \"hay\", \"tienen\"]\n",
    "        inventory_keywords_en = [\"inventory\", \"stock\", \"quantity\", \"how many\", \"available\", \"are there\", \"do you have\", \"have\"]\n",
    "        order_keywords_es = [\"ordenar\", \"pedir\", \"comprar\", \"quiero\", \"necesito\"]\n",
    "        order_keywords_en = [\"order\", \"buy\", \"purchase\", \"i want\", \"i need\"]\n",
    "        thanks_keywords_es = [\"gracias\", \"muchas gracias\", \"te lo agradezco\", \"agradecido\", \"gracie\"]\n",
    "        thanks_keywords_en = [\"thank\", \"thanks\", \"thank you\", \"appreciated\", \"grateful\", \"thx\"]\n",
    "\n",
    "\n",
    "        # --- Intent Detection & Context Handling ---\n",
    "        medicine_context = None\n",
    "        location_context = None\n",
    "        intent = \"unknown\"\n",
    "\n",
    "        try: # Specific try block for intent/context section\n",
    "            user_content_lower = user_content.lower() # Should be safe now\n",
    "            is_thanks = any(keyword in user_content_lower for keyword in (thanks_keywords_es if language == \"es\" else thanks_keywords_en))\n",
    "            is_hours = any(keyword in user_content_lower for keyword in (hours_keywords_es if language == \"es\" else hours_keywords_en))\n",
    "\n",
    "            # --- Helper Functions ---\n",
    "            # Ensure these access the globally defined lists correctly\n",
    "            def extract_medicine(text):\n",
    "                if not isinstance(text, str): return None\n",
    "                text_lower = text.lower()\n",
    "                for med in known_medicines: # Uses global known_medicines\n",
    "                    if med in text_lower: return med\n",
    "                return None\n",
    "\n",
    "            def extract_location(text):\n",
    "                if not isinstance(text, str): return None\n",
    "                text_lower = text.lower()\n",
    "                for loc in STORE_NAMES_ES: # Uses global STORE_NAMES_ES\n",
    "                    if loc in text_lower: return loc\n",
    "                return None\n",
    "            # --- End Helper Functions ---\n",
    "\n",
    "            current_medicine = extract_medicine(user_content)\n",
    "            current_location = extract_location(user_content)\n",
    "\n",
    "            # Context Recovery Logic\n",
    "            if not current_medicine and len(message_list) >= 3:\n",
    "                current_mentions_inventory = any(kw in user_content_lower for kw in inventory_keywords_es + inventory_keywords_en + order_keywords_es + order_keywords_en)\n",
    "                if current_location and current_mentions_inventory:\n",
    "                    prev_user_content = message_list[-3].get('content', '')\n",
    "                    if isinstance(prev_user_content, str):\n",
    "                        medicine_context = extract_medicine(prev_user_content)\n",
    "                        if medicine_context:\n",
    "                            print(f\"DEBUG: Context Recovery: Using medicine '{medicine_context}' from previous turn.\")\n",
    "                            current_medicine = medicine_context\n",
    "                    else:\n",
    "                        print(f\"WARNING: Previous user message content was not a string: {type(prev_user_content)}\")\n",
    "\n",
    "\n",
    "            # Determine Intent (Simplified)\n",
    "            is_inventory_kw = any(kw in user_content_lower for kw in inventory_keywords_es + inventory_keywords_en)\n",
    "            is_order_kw = any(kw in user_content_lower for kw in order_keywords_es + order_keywords_en)\n",
    "            is_location_kw = any(kw in user_content_lower for kw in location_keywords_es + location_keywords_en)\n",
    "            is_medication_kw = any(kw in user_content_lower for kw in medication_keywords_es + medication_keywords_en)\n",
    "\n",
    "            if current_medicine and is_order_kw: intent = \"order_request\"\n",
    "            elif current_medicine and is_inventory_kw: intent = \"inventory_check\"\n",
    "            elif is_hours: intent = \"hours_info\"\n",
    "            elif is_location_kw and not current_medicine: intent = \"location_info\"\n",
    "            elif is_medication_kw: intent = \"medication_info\"\n",
    "            else: # Fallback context check\n",
    "                if current_location and not current_medicine and len(message_list) >=3:\n",
    "                     prev_bot_msg_lower = last_bot_message.lower() # Safe now\n",
    "                     if 'en stock' in prev_bot_msg_lower or 'in stock' in prev_bot_msg_lower:\n",
    "                         intent = \"inventory_check\"\n",
    "                         if not medicine_context:\n",
    "                              prev_user_content = message_list[-3].get('content', '')\n",
    "                              if isinstance(prev_user_content, str):\n",
    "                                   medicine_context = extract_medicine(prev_user_content)\n",
    "                                   if medicine_context: current_medicine = medicine_context\n",
    "                              else:\n",
    "                                   print(f\"WARNING: Previous user message content was not a string during fallback: {type(prev_user_content)}\")\n",
    "\n",
    "            if intent == \"unknown\": intent = \"off_topic\"\n",
    "            print(f\"DEBUG: Intent={intent}, Medicine={current_medicine}, Location={current_location}\")\n",
    "\n",
    "        except AttributeError as ae:\n",
    "            print(f\"ERROR: AttributeError during Intent/Context processing: {ae}\")\n",
    "            print(traceback.format_exc())\n",
    "            log_interaction(query=user_content, response=\"AttributeError during intent processing\", query_type=\"intent_attr_error\", error=ae)\n",
    "            raise # Re-raise error\n",
    "        except NameError as ne: # Specifically catch NameError here\n",
    "             print(f\"ERROR: NameError during Intent/Context processing - likely missing global list (known_medicines or STORE_NAMES_ES?): {ne}\")\n",
    "             print(traceback.format_exc())\n",
    "             log_interaction(query=user_content, response=\"NameError during intent processing\", query_type=\"intent_name_error\", error=ne)\n",
    "             raise # Re-raise error\n",
    "\n",
    "\n",
    "        # --- ROUTING BASED ON DETERMINED INTENT ---\n",
    "        # (The rest of the routing logic remains the same as the previous version)\n",
    "        # ... (Handle Thanks) ...\n",
    "        # ... (Handle Order Follow-up) ...\n",
    "        # ... (Handle Inventory/Order) ...\n",
    "        # ... (Handle Location Info) ...\n",
    "        # ... (Handle Hours) ...\n",
    "        # ... (Handle RAG Medication Info - with its own debugging) ...\n",
    "        # ... (Handle Off-Topic) ...\n",
    "        # ... (Fallback Warning) ...\n",
    "\n",
    "        # Placeholder for the rest of the routing logic from the previous version\n",
    "        # Ensure each path returns a response string\n",
    "        if intent == \"hours_info\":\n",
    "             response = \"Todas nuestras farmacias están abiertas de 5 AM a 10 PM...\" if language == \"es\" else \"All our pharmacies are open from 5 AM to 10 PM...\"\n",
    "             log_interaction(query=user_content, response=response, query_type=\"hours_fixed\", duration_ms=0)\n",
    "             return response\n",
    "        # ... Add other intent handling blocks here, ensuring they return ...\n",
    "\n",
    "        # Make sure the RAG block is here\n",
    "        if intent == \"medication_info\":\n",
    "            print(f\"DEBUG: Routing to RAG system...\")\n",
    "            if not retriever:\n",
    "                 print(\"ERROR: Retriever is not available.\")\n",
    "                 return \"Lo siento, no puedo buscar información sobre medicamentos en este momento.\" if language == \"es\" else \"Sorry, I cannot look up medication information right now.\"\n",
    "            try: # Specific try for RAG block\n",
    "                # ... (Detailed RAG logic with debugging from previous step) ...\n",
    "                 if isinstance(response_content, str):\n",
    "                    response = response_content\n",
    "                    # ... (log and return response) ...\n",
    "                    log_interaction(query=user_content, response=response, query_type=\"medicine_rag\", duration_ms=0)\n",
    "                    return response\n",
    "                 else:\n",
    "                    # ... (handle error) ...\n",
    "                    raise TypeError(f\"Unexpected LLM response format. Expected string content, got {type(response_content)}\")\n",
    "            # ... (except blocks for RAG) ...\n",
    "            except AttributeError as ae: # Catch AttributeError specifically within RAG\n",
    "                 print(f\"ERROR: AttributeError during RAG processing: {ae}\")\n",
    "                 print(traceback.format_exc())\n",
    "                 log_interaction(query=user_content, response=\"AttributeError during RAG processing\", query_type=\"rag_attr_error\", error=ae)\n",
    "                 raise # Re-raise to be caught by outer handler\n",
    "            except Exception as e: # Catch other errors during RAG\n",
    "                 error_msg = \"Lo siento, tuve un problema al buscar información sobre ese medicamento.\" if language == \"es\" else \"I'm sorry, I had trouble finding information about that medication.\"\n",
    "                 print(f\"ERROR: Exception during RAG processing: {str(e)}\")\n",
    "                 print(traceback.format_exc())\n",
    "                 log_interaction(query=user_content, response=error_msg, query_type=\"medicine_rag_error\", duration_ms=0, error=str(e))\n",
    "                 return error_msg # Return specific RAG error message\n",
    "\n",
    "\n",
    "        # Default/Off-topic handling if no other intent matched or returned\n",
    "        print(f\"DEBUG: Handling as off-topic or fallback.\")\n",
    "        response = \"Lo siento, solo puedo ayudarte con preguntas sobre...\" if language == \"es\" else \"Sorry, I can only help with questions about...\" # Truncated\n",
    "        log_interaction(query=user_content, response=response, query_type=\"off_topic\", duration_ms=0)\n",
    "        return response\n",
    "\n",
    "\n",
    "    # --- Outer Exception Handler ---\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Uncaught exception in chat function: {e}\")\n",
    "        print(\"Traceback:\")\n",
    "        print(traceback.format_exc()) # Print detailed traceback\n",
    "        log_interaction(query=user_content, response=\"Generic Error Fallback\", query_type=\"chat_uncaught_error\", error=traceback.format_exc()) # Log full traceback\n",
    "        # Return the user-facing message, including the specific error string\n",
    "        return f\"I encountered an error: {str(e)}. Please try rephrasing your question.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f99f08-de3f-43ba-9851-504e4fcbb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_message_structure(message_list):\n",
    "    \"\"\"\n",
    "    Función de diagnóstico que muestra la estructura completa del message_list\n",
    "    \"\"\"\n",
    "    import json\n",
    "    print(\"==== DEBUG: MESSAGE STRUCTURE ====\")\n",
    "    print(f\"Type of message_list: {type(message_list)}\")\n",
    "    print(f\"Length of message_list: {len(message_list) if isinstance(message_list, list) else 'Not a list'}\")\n",
    "    \n",
    "    # Intenta serializar a JSON para una visualización fácil\n",
    "    try:\n",
    "        print(f\"Content: {json.dumps(message_list, indent=2)}\")\n",
    "    except:\n",
    "        # Si no se puede serializar, muestra elemento por elemento\n",
    "        if isinstance(message_list, list):\n",
    "            for i, item in enumerate(message_list):\n",
    "                print(f\"Item {i}:\")\n",
    "                print(f\"  Type: {type(item)}\")\n",
    "                print(f\"  Content: {str(item)[:100]}...\")\n",
    "        else:\n",
    "            print(f\"Raw content: {str(message_list)[:100]}...\")\n",
    "    \n",
    "    print(\"==== END DEBUG ====\")\n",
    "    \n",
    "    # Retorna un mensaje genérico para facilitar la depuración\n",
    "    return \"Diagnóstico completado. Por favor revisa los logs para ver la estructura del mensaje.\"\n",
    "\n",
    "def minimal_chat(message_list):\n",
    "    \"\"\"\n",
    "    Versión mínima de la función chat que solo maneja consultas sobre fiebre.\n",
    "    Elimina la mayoría de la complejidad para enfocarse en resolver el problema principal.\n",
    "    \"\"\"\n",
    "    import traceback\n",
    "    \n",
    "    try:\n",
    "        # Diagnóstico de la estructura del mensaje\n",
    "        debug_message_structure(message_list)\n",
    "        \n",
    "        # Retorna un mensaje fijo para pruebas\n",
    "        RESPUESTA_FIEBRE = \"\"\"\n",
    "        Para la fiebre, tenemos los siguientes medicamentos:\n",
    "        \n",
    "        1. Paracetamol (Acetaminofén) - Tylenol, Tempra\n",
    "           - Reduce la fiebre y alivia el dolor\n",
    "           - Disponible en tabletas, jarabe y gotas\n",
    "        \n",
    "        2. Ibuprofeno - Advil, Motrin\n",
    "           - Antiinflamatorio que reduce la fiebre y el dolor\n",
    "           - Disponible en tabletas y suspensión\n",
    "        \n",
    "        3. Naproxeno - Aleve\n",
    "           - Antiinflamatorio de acción prolongada\n",
    "           - Para adultos\n",
    "        \n",
    "        4. Ácido Acetilsalicílico - Aspirina\n",
    "           - Reduce la fiebre y la inflamación\n",
    "           - Solo para adultos, no recomendado para niños\n",
    "        \n",
    "        ¿Deseas información más específica sobre alguno de estos medicamentos?\n",
    "        \"\"\"\n",
    "        \n",
    "        return RESPUESTA_FIEBRE\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR EN MINIMAL_CHAT: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return f\"Error diagnosticado: {str(e)}\"\n",
    "\n",
    "def chat(message_list):\n",
    "    \"\"\"\n",
    "    Función chat modificada que intercepta el comando directamente para probar\n",
    "    una respuesta fija sin procesar el message_list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Si no hay mensaje, devolver mensaje genérico\n",
    "        if not message_list:\n",
    "            return \"Por favor, dime algo.\"\n",
    "            \n",
    "        # Intentar extraer el mensaje del usuario\n",
    "# Intentar extraer el mensaje del usuario\n",
    "        try:\n",
    "            user_message = \"\"\n",
    "            for msg in reversed(message_list):\n",
    "                if isinstance(msg, dict) and msg.get('role') == 'user':\n",
    "                    content = msg.get('content', '')\n",
    "                    if isinstance(content, str):\n",
    "                        user_message = content\n",
    "                    elif isinstance(content, list):\n",
    "                        user_message = \" \".join(str(x) for x in content)\n",
    "                    else:\n",
    "                        user_message = str(content)\n",
    "                    break\n",
    "        \n",
    "            user_message_lower = user_message.lower() if isinstance(user_message, str) else \"\"\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al extraer mensaje del usuario: {str(e)}\")\n",
    "        \n",
    "        # Si encontramos el mensaje y contiene \"fiebre\"\n",
    "        user_message_lower = user_message.lower() if isinstance(user_message, str) else \"\"\n",
    "        \n",
    "        # Responder a consultas sobre fiebre ignorando todo el procesamiento complejo\n",
    "        if \"fiebre\" in user_message_lower:\n",
    "            return minimal_chat(message_list)\n",
    "        else:\n",
    "            # Para otros mensajes, enviar respuesta genérica\n",
    "            return f\"Recibí tu mensaje. Para consultas sobre medicamentos específicos como los de fiebre, por favor menciona el síntoma o tipo de medicamento que buscas.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ERROR EN CHAT PRINCIPAL: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return f\"Ocurrió un error al procesar tu mensaje: {str(e)}. Por favor, intenta con otra pregunta.\"\n",
    "\n",
    "\n",
    "# Implementación de chat alternativa si la principal falla\n",
    "def fallback_chat(message_list):\n",
    "    \"\"\"\n",
    "    Implementación alternativa extremadamente simple para garantizar alguna respuesta\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mensaje fijo para solución inmediata\n",
    "        return \"\"\"\n",
    "        Para la fiebre, tenemos varios medicamentos como Paracetamol (Tylenol), \n",
    "        Ibuprofeno (Advil), Naproxeno (Aleve) y Aspirina. \n",
    "        Todos están disponibles en nuestras farmacias.\n",
    "        \"\"\"\n",
    "    except:\n",
    "        return \"Lo siento, estamos experimentando dificultades técnicas. Por favor, intenta más tarde.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289b7d59-0b62-45ad-ba85-e6a0657cd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LangChain implementation without typing\n",
    "\n",
    "# Session storage for maintaining conversation history\n",
    "session_histories = {}\n",
    "\n",
    "# Define the models/LLMs\n",
    "# Assuming MODEL is defined elsewhere in your code\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "agent_llm = ChatOpenAI(model=MODEL, temperature=0)\n",
    "\n",
    "# Load the vectorstore\n",
    "if 'vectorstore' not in locals() or vectorstore is None:\n",
    "    print(\"Loading existing vectorstore from disk...\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    try:\n",
    "        vectorstore = Chroma(persist_directory=db_name, embedding_function=embeddings)\n",
    "        print(f\"Loaded vectorstore with {vectorstore._collection.count()} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading vectorstore: {e}\")\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Function to get or create session history\n",
    "def get_session_history(session_id):\n",
    "    if session_id not in session_histories:\n",
    "        session_histories[session_id] = ChatMessageHistory()\n",
    "    return session_histories[session_id]\n",
    "\n",
    "# Create a RAG chain\n",
    "def create_rag_chain():\n",
    "    # Define the prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"\"\"You are an expert pharmaceutical assistant. Use the following context to answer the question.\n",
    "        \n",
    "If you're asked about side effects, focus on the information in the 'Side Effects (Common)' and 'Side Effects (Rare)' fields.\n",
    "If you're asked about stores or inventory, explain that this information needs to be queried from the database.\n",
    "Answer the question based only on the provided context. If the information isn't available, say so clearly.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessage(content=\"{question}\"),\n",
    "        SystemMessage(content=\"Context: {context}\")\n",
    "    ])\n",
    "    \n",
    "    # Create the RAG chain\n",
    "    return (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough(), \"chat_history\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = create_rag_chain()\n",
    "\n",
    "# Define the SQL query tool\n",
    "@tool\n",
    "def sql_query(query):\n",
    "    \"\"\"Execute a SQL query against the store and inventory database\"\"\"\n",
    "    try:\n",
    "        # Assuming sql_agent is defined elsewhere\n",
    "        return sql_agent.invoke({\"input\": query})[\"output\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error querying database: {str(e)}\"\n",
    "\n",
    "# Vector search function for direct access to RAG\n",
    "def vector_search(query, session_id=\"default\"):\n",
    "    try:\n",
    "        # Get history and pass it explicitly\n",
    "        history = get_session_history(session_id)\n",
    "        result = rag_chain.invoke({\"question\": query, \"chat_history\": history.messages})\n",
    "        \n",
    "        # Record the exchange\n",
    "        history.add_user_message(query)\n",
    "        history.add_ai_message(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in vector_search: {e}\")\n",
    "        # Fallback to basic query without history\n",
    "        return rag_chain.invoke({\"question\": query, \"chat_history\": []})\n",
    "\n",
    "# Simple agent without LangGraph\n",
    "def simple_agent(query):\n",
    "    \"\"\"A simple agent implementation that doesn't use LangGraph\"\"\"\n",
    "    # Create a prompt for the agent\n",
    "    agent_prompt = f\"\"\"You are a helpful assistant that can answer questions about medicines and store inventory.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "If this is about store inventory, locations, or similar store-related information, use the SQL database.\n",
    "If this is about medicine properties, side effects, or drug information, use the medicine information database.\n",
    "Otherwise, answer directly.\n",
    "\n",
    "Respond with your final answer.\"\"\"\n",
    "\n",
    "    # Get a response from the LLM\n",
    "    response = agent_llm.invoke(agent_prompt)\n",
    "    \n",
    "    # Extract the content\n",
    "    if hasattr(response, \"content\"):\n",
    "        return response.content\n",
    "    else:\n",
    "        return str(response)\n",
    "\n",
    "# The integrated chat function\n",
    "def chat(question, session_id=\"default\"):\n",
    "    try:\n",
    "        # For store-related questions, directly route to SQL agent\n",
    "        if any(keyword in question.lower() for keyword in [\"store\", \"stores\", \"location\", \"locations\", \"inventory\", \"stock\", \"how many\"]):\n",
    "            try:\n",
    "                print(f\"Routing to SQL agent: {question}\")\n",
    "                result = sql_query(question)\n",
    "                \n",
    "                # Record the exchange in history\n",
    "                history = get_session_history(session_id)\n",
    "                history.add_user_message(question)\n",
    "                history.add_ai_message(result)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"SQL direct routing failed: {e}, falling back to simple agent\")\n",
    "        \n",
    "        # For medicine-related questions about side effects, use RAG\n",
    "        if any(keyword in question.lower() for keyword in [\"side effect\", \"medicine\", \"drug\", \"medication\"]):\n",
    "            try:\n",
    "                print(f\"Routing to RAG chain: {question}\")\n",
    "                return vector_search(question, session_id)\n",
    "            except Exception as e:\n",
    "                print(f\"RAG chain failed: {e}, falling back to simple agent\")\n",
    "        \n",
    "        # For general questions, use the simple agent\n",
    "        try:\n",
    "            print(f\"Using simple agent: {question}\")\n",
    "            result = simple_agent(question)\n",
    "            \n",
    "            # Record the exchange in history\n",
    "            history = get_session_history(session_id)\n",
    "            history.add_user_message(question)\n",
    "            history.add_ai_message(result)\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Agent failed: {e}, falling back to direct LLM\")\n",
    "            try:\n",
    "                response = llm.invoke(question)\n",
    "                content = response.content if hasattr(response, \"content\") else str(response)\n",
    "                return content\n",
    "            except Exception as llm_err:\n",
    "                return f\"I encountered several errors processing your request. Please try rephrasing your question. Error: {str(llm_err)}\"\n",
    "                \n",
    "    except Exception as e:\n",
    "        return f\"I encountered an error: {str(e)}. Please try rephrasing your question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d29aab-8fec-470f-b5cb-f10d8da39ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "# Optionally, view the first chunk\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0ef209-d389-4e1c-b437-ed923bf4dd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 626 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a6fd7f-f378-445b-8e11-571cd62156a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_consolidated_interface():\n",
    "    \"\"\"Creates a simplified Gradio interface with language selection buttons\"\"\"\n",
    "    # Make sure gr.update is available\n",
    "    try:\n",
    "        gr.update\n",
    "    except AttributeError:\n",
    "        from gradio import update as gr_update\n",
    "        gr.update = gr_update\n",
    "        \n",
    "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# Farma AI Panama\")\n",
    "        gr.Markdown(\"Asistente virtual para consultas sobre farmacias, horarios, medicamentos e inventario.\")\n",
    "\n",
    "        # States\n",
    "        language_state = gr.State(\"asking\")\n",
    "        show_chat_interface = gr.State(False)\n",
    "        \n",
    "        # Initial greeting\n",
    "        initial_greeting = \"¡Hola! Soy el asistente virtual de Farma AI Panama. Por favor selecciona tu idioma preferido: / Hello! I'm Farma AI Panama's virtual assistant. Please select your preferred language:\"\n",
    "\n",
    "        # Main chat interface - using the messages format instead of deprecated tuples\n",
    "        chatbot = gr.Chatbot(\n",
    "            height=500,\n",
    "            value=[{\"role\": \"assistant\", \"content\": initial_greeting}],\n",
    "            type=\"messages\"  # Use messages format instead of tuples\n",
    "        )\n",
    "        \n",
    "        # Function to handle Spanish language selection\n",
    "        def select_spanish(history, chat_visible):\n",
    "            response = \"Has seleccionado español. ¿En qué puedo ayudarte hoy?\"\n",
    "            new_history = history + [{\"role\": \"assistant\", \"content\": response}]\n",
    "            return new_history, \"es\", True\n",
    "        \n",
    "        # Function to handle English language selection\n",
    "        def select_english(history, chat_visible):\n",
    "            response = \"You've selected English. How can I help you today?\"\n",
    "            new_history = history + [{\"role\": \"assistant\", \"content\": response}]\n",
    "            return new_history, \"en\", True\n",
    "        \n",
    "        # Function to handle message submission\n",
    "        def handle_chat(message, history, lang):\n",
    "            if not message.strip():\n",
    "                return \"\", history\n",
    "            \n",
    "            # Add the current user message\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            \n",
    "            try:\n",
    "                # Prepare system prompt for the chat function\n",
    "                system_prompt = {\"role\": \"system\", \"content\": f\"User prefers: {lang}. Respond in {'Spanish' if lang == 'es' else 'English'}.\"}\n",
    "                \n",
    "                # Convert to the format expected by our chat function\n",
    "                messages_for_chat = [system_prompt]\n",
    "                for msg in history:\n",
    "                    messages_for_chat.append(msg)\n",
    "                \n",
    "                # Call the chat function\n",
    "                response = chat(messages_for_chat)\n",
    "                \n",
    "                # Add the bot's response to history\n",
    "                history.append({\"role\": \"assistant\", \"content\": response})\n",
    "                \n",
    "                return \"\", history\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(f\"Error in chat function: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "                # Provide an error message based on the user's language\n",
    "                error_msg = \"Lo siento, ocurrió un error interno.\" if lang == \"es\" else \"Sorry, an internal error occurred.\"\n",
    "                \n",
    "                # Add the error response to history\n",
    "                history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "                \n",
    "                return \"\", history\n",
    "        \n",
    "        # Function to clear chat history\n",
    "        def clear_chat(lang_state, chat_visible):\n",
    "            if lang_state == \"asking\":\n",
    "                return [{\"role\": \"assistant\", \"content\": initial_greeting}], \"asking\", False\n",
    "            else:\n",
    "                # Just clear the messages but keep the chat interface\n",
    "                reset_msg = \"¿En qué puedo ayudarte hoy?\" if lang_state == \"es\" else \"How can I help you today?\"\n",
    "                return [{\"role\": \"assistant\", \"content\": reset_msg}], lang_state, True\n",
    "        \n",
    "        # Create the layout with both language selection and chat input interfaces\n",
    "        with gr.Row() as language_buttons_row:\n",
    "            spanish_btn = gr.Button(\"Español\", variant=\"primary\")\n",
    "            english_btn = gr.Button(\"English\", variant=\"primary\")\n",
    "        \n",
    "        # Spanish input interface\n",
    "        with gr.Row(visible=False) as spanish_input_row:\n",
    "            spanish_msg = gr.Textbox(placeholder=\"Escribe aquí...\", show_label=False, scale=4)\n",
    "            spanish_send_btn = gr.Button(\"Enviar\", variant=\"primary\", scale=1)\n",
    "        \n",
    "        # English input interface\n",
    "        with gr.Row(visible=False) as english_input_row:\n",
    "            english_msg = gr.Textbox(placeholder=\"Type here...\", show_label=False, scale=4)\n",
    "            english_send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "        \n",
    "        # Clear button row\n",
    "        with gr.Row():\n",
    "            clear_btn = gr.Button(\"Borrar conversación / Clear chat\")\n",
    "        \n",
    "        # Update visibility function\n",
    "        def update_interface_visibility(show_chat, lang):\n",
    "            # Hide language buttons if chat interface should be shown\n",
    "            lang_buttons_visible = not show_chat\n",
    "            \n",
    "            # Show appropriate language input based on selection\n",
    "            spanish_input_visible = show_chat and lang == \"es\"\n",
    "            english_input_visible = show_chat and lang == \"en\"\n",
    "            \n",
    "            # Update button text based on language\n",
    "            clear_button_text = \"Borrar conversación\" if lang == \"es\" else \"Clear chat\"\n",
    "            \n",
    "            # Return Gradio update objects for each component\n",
    "            return (\n",
    "                gr.update(visible=lang_buttons_visible),  # For language_buttons_row\n",
    "                gr.update(visible=spanish_input_visible), # For spanish_input_row\n",
    "                gr.update(visible=english_input_visible), # For english_input_row\n",
    "                clear_button_text                         # This can be a direct value\n",
    "            )\n",
    "        \n",
    "        # Connect event handlers\n",
    "        spanish_btn.click(\n",
    "            fn=select_spanish,\n",
    "            inputs=[chatbot, show_chat_interface],\n",
    "            outputs=[chatbot, language_state, show_chat_interface]\n",
    "        ).then(\n",
    "            fn=update_interface_visibility,\n",
    "            inputs=[show_chat_interface, language_state],\n",
    "            outputs=[\n",
    "                language_buttons_row,  # Pass the component itself\n",
    "                spanish_input_row,     # Pass the component itself  \n",
    "                english_input_row,     # Pass the component itself\n",
    "                clear_btn              # This can be updated directly\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        english_btn.click(\n",
    "            fn=select_english,\n",
    "            inputs=[chatbot, show_chat_interface],\n",
    "            outputs=[chatbot, language_state, show_chat_interface]\n",
    "        ).then(\n",
    "            fn=update_interface_visibility,\n",
    "            inputs=[show_chat_interface, language_state],\n",
    "            outputs=[\n",
    "                language_buttons_row,  # Pass the component itself\n",
    "                spanish_input_row,     # Pass the component itself\n",
    "                english_input_row,     # Pass the component itself\n",
    "                clear_btn              # This can be updated directly\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Connect chat input handlers for both languages\n",
    "        spanish_send_btn.click(\n",
    "            fn=handle_chat,\n",
    "            inputs=[spanish_msg, chatbot, language_state],\n",
    "            outputs=[spanish_msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        spanish_msg.submit(\n",
    "            fn=handle_chat,\n",
    "            inputs=[spanish_msg, chatbot, language_state],\n",
    "            outputs=[spanish_msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        english_send_btn.click(\n",
    "            fn=handle_chat,\n",
    "            inputs=[english_msg, chatbot, language_state],\n",
    "            outputs=[english_msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        english_msg.submit(\n",
    "            fn=handle_chat,\n",
    "            inputs=[english_msg, chatbot, language_state],\n",
    "            outputs=[english_msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        # Clear button handler\n",
    "        clear_btn.click(\n",
    "            fn=clear_chat,\n",
    "            inputs=[language_state, show_chat_interface],\n",
    "            outputs=[chatbot, language_state, show_chat_interface]\n",
    "        ).then(\n",
    "            fn=update_interface_visibility,\n",
    "            inputs=[show_chat_interface, language_state],\n",
    "            outputs=[\n",
    "                language_buttons_row,  # Pass the component itself\n",
    "                spanish_input_row,     # Pass the component itself\n",
    "                english_input_row,     # Pass the component itself\n",
    "                clear_btn              # This can be updated directly\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a469f57-1c3c-4f83-b8bf-89388ed5d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All components initialized successfully. Launching Gradio interface...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute '_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m db \u001b[38;5;129;01mand\u001b[39;00m llm \u001b[38;5;129;01mand\u001b[39;00m vectorstore \u001b[38;5;129;01mand\u001b[39;00m retriever \u001b[38;5;129;01mand\u001b[39;00m sql_agent:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll components initialized successfully. Launching Gradio interface...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     interface \u001b[38;5;241m=\u001b[39m create_consolidated_interface()\n\u001b[0;32m      9\u001b[0m     interface\u001b[38;5;241m.\u001b[39mlaunch(debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# share=True can expose it publicly\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[13], line 117\u001b[0m, in \u001b[0;36mcreate_consolidated_interface\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lang_buttons_visible, spanish_input_visible, english_input_visible, clear_button_text\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Connect event handlers\u001b[39;00m\n\u001b[0;32m    113\u001b[0m spanish_btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    114\u001b[0m     fn\u001b[38;5;241m=\u001b[39mselect_spanish,\n\u001b[0;32m    115\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[chatbot, show_chat_interface],\n\u001b[0;32m    116\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m[chatbot, language_state, show_chat_interface]\n\u001b[1;32m--> 117\u001b[0m )\u001b[38;5;241m.\u001b[39mthen(\n\u001b[0;32m    118\u001b[0m     fn\u001b[38;5;241m=\u001b[39mupdate_interface_visibility,\n\u001b[0;32m    119\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[show_chat_interface, language_state],\n\u001b[0;32m    120\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    121\u001b[0m         language_buttons_row\u001b[38;5;241m.\u001b[39mvisible,  \u001b[38;5;66;03m# Use the .visible property\u001b[39;00m\n\u001b[0;32m    122\u001b[0m         spanish_input_row\u001b[38;5;241m.\u001b[39mvisible,     \u001b[38;5;66;03m# Use the .visible property\u001b[39;00m\n\u001b[0;32m    123\u001b[0m         english_input_row\u001b[38;5;241m.\u001b[39mvisible,     \u001b[38;5;66;03m# Use the .visible property\u001b[39;00m\n\u001b[0;32m    124\u001b[0m         clear_btn                      \u001b[38;5;66;03m# This is fine to update directly\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     ]\n\u001b[0;32m    126\u001b[0m )\n\u001b[0;32m    128\u001b[0m english_btn\u001b[38;5;241m.\u001b[39mclick(\n\u001b[0;32m    129\u001b[0m     fn\u001b[38;5;241m=\u001b[39mselect_english,\n\u001b[0;32m    130\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[chatbot, show_chat_interface],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     ]\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Connect chat input handlers for both languages\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gradio\\events.py:709\u001b[0m, in \u001b[0;36mEventListener._setup.<locals>.event_trigger\u001b[1;34m(block, fn, inputs, outputs, api_name, scroll_to_output, show_progress, show_progress_on, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, js, concurrency_limit, concurrency_id, show_api, time_limit, stream_every, like_user_message)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _callback:\n\u001b[0;32m    708\u001b[0m     _callback(block)\n\u001b[1;32m--> 709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dependency(block, dep\u001b[38;5;241m.\u001b[39mget_config(), dep_index, fn)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py:605\u001b[0m, in \u001b[0;36mBlockFunction.get_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets,\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [block\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs],\n\u001b[1;32m--> 605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [block\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs],\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjs,\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue,\n\u001b[0;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_name,\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscroll_to_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscroll_to_output,\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress,\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_progress_on\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m [block\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress_on],\n\u001b[0;32m    615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    616\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size,\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancels\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancels,\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m    619\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes_generator,\n\u001b[0;32m    620\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cancel_function,\n\u001b[0;32m    621\u001b[0m         },\n\u001b[0;32m    622\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollects_event_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollects_event_data,\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_after\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_after,\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_only_on_success\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_only_on_success,\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrigger_mode,\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_api\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_api,\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendered_in\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrendered_in\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrendered_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderable\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderable \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection,\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_limit,\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_every\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_every,\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlike_user_message\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlike_user_message,\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_specific_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_specific_args,\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjs_implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__js_implementation__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    635\u001b[0m     }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute '_id'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Main Execution Block\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure critical components are available before launching UI\n",
    "    if engine and db and llm and vectorstore and retriever and sql_agent:\n",
    "        print(\"All components initialized successfully. Launching Gradio interface...\")\n",
    "        interface = create_consolidated_interface()\n",
    "        interface.launch(debug=True, share=False) # share=True can expose it publicly\n",
    "    else:\n",
    "        print(\"ERROR: Could not initialize all required components. Aborting launch.\")\n",
    "        log_interaction(query=\"Launch Check\", response=\"Aborted\", query_type=\"launch_error\", error=\"Missing components\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
